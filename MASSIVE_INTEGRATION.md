## üåê Massive.com Flat File Data Integration

### Overview

The system now supports **Massive.com** (formerly Polygon) as an alternative data source for daily stock data via S3-compatible flat files. This provides an enterprise-grade alternative to Yahoo Finance with better data quality and reliability.

### Features

‚úÖ **Seamless Integration**: Drop-in replacement for yfinance  
‚úÖ **Automatic Fallback**: Falls back to yfinance if Massive data unavailable  
‚úÖ **Cache Compatible**: Works with existing cache system  
‚úÖ **Same Format**: Returns data in yfinance-compatible DataFrame format  
‚úÖ **Daily Data Only**: Optimized for end-of-day analysis  

### Setup

#### 1. Credentials Configuration

Your Massive.com credentials are stored in `~/.aws/credentials` under the `[massive]` profile:

```ini
[massive]
aws_access_key_id = 572c8693-717a-4773-854f-142470c3f7c1
aws_secret_access_key = sdv27L3qI5tfOTdU0wT4EQwuqKU6eops
```

#### 2. S3 Endpoint Configuration

```python
Endpoint: https://files.massive.com
Bucket: flatfiles
Prefix: us_stocks_sip/day_aggs_v1/
```

### Usage

#### Basic Usage

```bash
# Use Massive.com as data source (daily data only)
python vol_analysis.py AAPL --data-source massive

# Falls back to yfinance if Massive data unavailable
python vol_analysis.py AAPL -p 6mo --data-source massive

# Default remains yfinance
python vol_analysis.py AAPL  # Uses yfinance
```

#### Programmatic Usage

```python
from data_manager import get_smart_data

# Fetch from Massive.com
df = get_smart_data('AAPL', '6mo', data_source='massive')

# Fetch from yfinance (default)
df = get_smart_data('AAPL', '6mo', data_source='yfinance')
```

### Testing

**Two-Part Test Suite** (Recommended):

```bash
# Part 1: File listing (proves connectivity)
python test_massive_part1_list.py

# Part 2: File download (proves data access)
python test_massive_part2_download.py

# Historical integration (end-to-end test with ticker T)
python test_massive_historical.py
```

**Comprehensive Test Suite:**
```bash
python test_massive_integration.py
```

**All test scripts use the 'massive' profile from ~/.aws/credentials** - no hardcoded credentials.

**Expected Results:**
- ‚úÖ Part 1: Lists available files successfully
- ‚úÖ Part 2: Downloads historical data (March 2024)

### Data Availability

**Confirmed Working** (Tested 2025-11-11):

‚úÖ **Historical Data Access**: Successfully retrieved March 2024 data  
‚úÖ **Daily Aggregates**: `day_aggs_v1` files work perfectly  
‚úÖ **Real Market Data**: Ticker T (AT&T) - 20 trading days, full OHLCV  
‚úÖ **File Downloads**: 200KB compressed files download in ~2 seconds  

**Known Limitations**:

‚ö†Ô∏è **Recent Data (Nov 2025)**: Returns 403 Forbidden  
- Your subscription includes historical data
- Recent data (current month) may not be accessible
- Data lag appears to be several months

**Automatic Fallback**: System falls back to yfinance when:
- Requested dates unavailable in Massive flat files
- 403 Forbidden errors (recent data not accessible)
- Any S3 connection issues
- Invalid ticker symbols

### Architecture

```
get_smart_data()
    ‚îú‚îÄ‚îÄ data_source == "massive"
    ‚îÇ   ‚îú‚îÄ‚îÄ Try: massive_data_provider.get_massive_daily_data()
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Initialize S3 client with Massive endpoint
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Download flat files for date range
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Parse CSV.GZ files
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Return DataFrame
    ‚îÇ   ‚îî‚îÄ‚îÄ Except: Fall back to yfinance
    ‚îî‚îÄ‚îÄ data_source == "yfinance" (default)
        ‚îî‚îÄ‚îÄ Use existing yfinance logic
```

### File Structure

```
massive_data_provider.py       # S3 client and data provider
data_manager.py                 # Enhanced with data_source parameter
test_massive_integration.py     # Test suite
~/.aws/credentials              # Credentials storage
```

### Flat File Organization

Massive.com organizes daily aggregate files by date:

```
s3://flatfiles/us_stocks_sip/day_aggs_v1/
    ‚îî‚îÄ‚îÄ YYYY/
        ‚îî‚îÄ‚îÄ MM/
            ‚îî‚îÄ‚îÄ YYYY-MM-DD.csv.gz
```

Each file contains all tickers for that trading day:
```csv
ticker,volume,open,close,high,low,window_start,transactions
AAPL,4930,200.29,200.5,200.63,200.29,1744792500000000000,129
MSFT,1815,200.39,200.34,200.61,200.34,1744792560000000000,57
...
```

### Advantages Over yfinance

‚úÖ **Enterprise Quality**: Professional-grade market data  
‚úÖ **Better Reliability**: No rate limiting or throttling  
‚úÖ **Bulk Access**: Download entire trading days efficiently  
‚úÖ **S3 Infrastructure**: Leverages AWS S3 reliability  
‚úÖ **Historical Archive**: Comprehensive historical data  

### Limitations

‚ö†Ô∏è **Daily Data Only**: No intraday intervals supported  
‚ö†Ô∏è **Subscription Required**: Requires active Massive.com subscription  
‚ö†Ô∏è **Date Lag**: Recent dates (T-1, T-2) may not be available yet  
‚ö†Ô∏è **Ticker Coverage**: Limited to US stocks (SIP feed)  

### Troubleshooting

#### 403 Forbidden Errors

```
ERROR: S3 error downloading: (403) Forbidden
```

**Causes:**
- Recent data not yet available
- Subscription doesn't include requested data
- Credentials expired

**Solution:** System automatically falls back to yfinance

#### Connection Failures

```bash
# Test connection
python -c "from massive_data_provider import test_massive_connection; test_massive_connection()"
```

#### No Data Retrieved

```bash
# Check available date range
python -c "
from massive_data_provider import MassiveDataProvider
provider = MassiveDataProvider()
# List available files
"
```

### Bulk Cache Population (RECOMMENDED)

For efficiently populating cache with historical data across many tickers, use the **bulk download approach** instead of per-ticker downloads.

#### **Why Bulk is Better**

**Per-Ticker Approach** (`populate_cache.py --data-source massive`):
- Downloads the same daily file multiple times (once per ticker)
- For 40 tickers √ó 500 days = 20,000 file downloads
- Estimated time: ~139 hours

**Bulk Approach** (`populate_cache_bulk.py`):
- Downloads each daily file once (contains all US stocks)
- Split file by ticker after download
- For 500 days = 500 file downloads
- Estimated time: ~8 minutes
- **40x faster!**

#### **How Bulk Processing Works**

```
1. Download daily file (2025-11-11.csv.gz)
   ‚Üì Contains ALL ~11,500 US stocks for that day
   
2. Parse and split by tickers
   ‚Üì Our 53 tracked tickers ‚Üí data_cache/
   ‚Üì Other 11,447 tickers ‚Üí massive_cache/ (compressed)
   
3. Repeat for each trading day
   ‚Üì Smart duplicate detection skips already-cached dates
   
4. Result: Complete historical cache in minutes
```

#### **Two-Tier Caching Strategy**

**`data_cache/` Directory:**
- Your tracked tickers (from stocks.txt, ibd.txt, etc.)
- Uncompressed CSV format
- Ready for immediate use
- Example: `data_cache/AAPL_1d_data.csv`

**`massive_cache/` Directory:**
- All other US stocks (~11,500 tickers)
- Compressed .csv.gz format
- Can extract any ticker later without re-downloading
- Example: `massive_cache/2025-11-11.csv.gz`

#### **Usage Examples**

```bash
# Basic: Populate last 24 months
python populate_cache_bulk.py --months 24

# Specific date range
python populate_cache_bulk.py --start 2024-01-01 --end 2024-12-31

# Skip saving other tickers (faster, less storage)
python populate_cache_bulk.py --months 12 --no-save-others
```

#### **Sectional Population**

Build cache incrementally with automatic duplicate detection:

```bash
# Week 1: Start with 1 month
python populate_cache_bulk.py --months 1
# Result: ~21 trading days cached

# Week 2: Extend to 6 months (automatically skips month 1)
python populate_cache_bulk.py --months 6
# Result: Downloads only months 2-6

# Month 1: Full 24 months (skips months 1-6)
python populate_cache_bulk.py --months 24
# Result: Downloads only months 7-24
```

**Key Advantage**: No redundant downloads. The script checks existing cache files and only downloads missing dates.

#### **Performance Benchmarks**

| Duration | Trading Days | Download Time | Process Time | Total Time |
|----------|-------------|---------------|--------------|------------|
| 1 month | ~21 days | ~6 seconds | ~24 seconds | **~30 seconds** |
| 6 months | ~130 days | ~36 seconds | ~2 minutes | **~2.5 minutes** |
| 12 months | ~250 days | ~70 seconds | ~4 minutes | **~5 minutes** |
| 24 months | ~500 days | ~140 seconds | ~7 minutes | **~8 minutes** |

*Based on actual test results with 53 tracked tickers*

#### **Storage Requirements**

For 24 months of data (500 trading days):

**data_cache/** (your tickers):
- 53 tickers √ó 500 days = 26,500 data points
- Uncompressed CSV: ~25 MB
- One file per ticker

**massive_cache/** (all others):
- 500 daily files √ó ~0.22 MB each
- Compressed .csv.gz: ~110 MB
- ~11,500 tickers per file

**Total storage**: ~135 MB for complete 24-month archive

### Future Enhancements

Potential improvements for future releases:

- [x] Bulk download mode (IMPLEMENTED)
- [x] Sectional population with duplicate detection (IMPLEMENTED)
- [x] Two-tier caching for tracked vs all tickers (IMPLEMENTED)
- [ ] Minute/hour aggregate support
- [ ] Options data integration
- [ ] Crypto data support
- [ ] Automatic subscription tier detection
- [ ] Smart date range selection based on availability
- [ ] Parallel file downloads for speed

### Support

For Massive.com API issues:
- Dashboard: https://massive.com/dashboard
- Documentation: https://massive.com/docs/flat-files
- Support: https://massive.com/contact

For integration issues:
- Run test suite: `python test_massive_integration.py`
- Check logs for detailed error messages
- Verify AWS credentials configuration

---

**Note**: This integration provides professional-grade data access while maintaining full backward compatibility with the existing yfinance-based system.
